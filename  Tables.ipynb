{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f5d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a352ba1d",
   "metadata": {},
   "source": [
    "# Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa61bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHIPS_variable_trend(basin, TargetName):       \n",
    "    ships = pd.read_csv('data/SHIPS/SHIPS_development_%s.csv'%basin)\n",
    "    ships = ships.replace(9999,np.NaN)\n",
    "    ships['DATE'] = ships['DATE'].apply(lambda x: str(x).zfill(6))   #fills date to 6 digits with 0\n",
    "    ships['YEAR'] = ships['DATE'].apply(lambda x: ('19' + str(x)[0:2]) if (str(x)[0]=='8' or str(x)[0]=='9') else ('20' + str(x)[0:2])).astype(int)\n",
    "    ships['time'] = ships['DATE'].apply(lambda x: ('19' + str(x)) if (str(x)[0]=='8' or str(x)[0]=='9') else ('20' + str(x))) + ships['HOUR'].apply(lambda x: str(x).zfill(2))\n",
    "    ships['time'] = ships['time'].apply(lambda x: dt.datetime.strptime(x,'%Y%m%d%H'))\n",
    "    \n",
    "    if TargetName != 'VMAX':\n",
    "        ships = ships[['YEAR', 'ID', 'time', 'VMAX', TargetName]]\n",
    "    else:\n",
    "        ships = ships[['YEAR', 'ID', 'time', 'VMAX']]\n",
    "    if TargetName == 'POT':                 # remove outliers\n",
    "        ships = ships[(ships['POT']>-9000)]\n",
    "    if TargetName == 'CFLX':                # remove outliers\n",
    "        ships = ships[(ships['CFLX']<5000)]\n",
    "    \n",
    "    ships_ID_unique = sorted(ships['ID'].unique())\n",
    "    ships_processed = pd.DataFrame(columns=['YEAR', 'ID', 'time', TargetName, 'type', 'duration', 'duration_type'])\n",
    "    for TC_ID in ships_ID_unique:\n",
    "        TC_data = ships[ships['ID'] == TC_ID].sort_values(by='time').reset_index(drop=True)\n",
    "        if len(TC_data) > 0:\n",
    "            if len(TC_data[TC_data['VMAX'] >= 34]) > 0:\n",
    "                start_idx = TC_data[TC_data['VMAX'] >= 34].index[0]\n",
    "                TC_ID_data = TC_data.iloc[start_idx:].reset_index(drop=True)\n",
    "                TC_duration = (TC_ID_data.iloc[-1]['time'] - TC_ID_data.iloc[0]['time']).total_seconds() / 3600 + 3\n",
    "                if TC_duration < 100:\n",
    "                    TC_duration_type = 'short'\n",
    "                elif TC_duration < 200:\n",
    "                    TC_duration_type = 'medium'\n",
    "                else:\n",
    "                    TC_duration_type = 'long'\n",
    "                LMI_idx = TC_ID_data['VMAX'].idxmax()\n",
    "                TC_ID_data['type'] = 'before'\n",
    "                TC_ID_data.loc[LMI_idx:, 'type'] = 'after'\n",
    "                TC_ID_data['duration'] = TC_duration\n",
    "                TC_ID_data['duration_type'] = TC_duration_type\n",
    "                TC_ID_data = TC_ID_data.dropna().reset_index(drop=True)\n",
    "                ships_processed = pd.concat([ships_processed, TC_ID_data[['YEAR', 'ID', 'time', TargetName, 'type', 'duration', 'duration_type']]], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "    def predictor_trend(df, col):   \n",
    "        save_folder = f'figures/SHIPS_TS_34/trends/drop_separate/{basin}'\n",
    "        title_str = 'each predictor dropnan separately'\n",
    "\n",
    "        # Target trend\n",
    "        annual_mean = df.groupby('YEAR').agg({col: ['mean', 'count']}).reset_index()\n",
    "        years = annual_mean['YEAR'].values.reshape(-1).astype(int)\n",
    "        values = annual_mean[(col, 'mean')].values.reshape(-1).astype(float)\n",
    "        num_years_all = len(years)\n",
    "\n",
    "        model = sm.OLS(values, sm.add_constant(years)).fit()\n",
    "        y_pred = model.predict(sm.add_constant(years))\n",
    "        slope = model.params[1]\n",
    "        intercept = model.params[0]\n",
    "        r2 = model.rsquared\n",
    "        p_value = model.pvalues[1]\n",
    "        ci = model.conf_int()[1]\n",
    "        ci_width = (ci[1] - ci[0]) / 2\n",
    "\n",
    "\n",
    "        # Target trend before LMI\n",
    "        annual_mean_before = df[df['type'] == 'before'].groupby('YEAR').agg({col: ['mean', 'count']}).reset_index()\n",
    "        years = annual_mean_before['YEAR'].values.reshape(-1)\n",
    "        values = annual_mean_before[(col, 'mean')].values.reshape(-1).astype(float)\n",
    "        num_years_before = len(years)\n",
    "\n",
    "        model_before = sm.OLS(values, sm.add_constant(years)).fit()\n",
    "        y_pred_before = model_before.predict(sm.add_constant(years))\n",
    "        slope_before = model_before.params[1]\n",
    "        intercept_before = model_before.params[0]\n",
    "        r2_before = model_before.rsquared\n",
    "        p_value_before = model_before.pvalues[1]\n",
    "        ci_before = model_before.conf_int()[1]\n",
    "        ci_width_before = (ci_before[1] - ci_before[0]) / 2\n",
    "\n",
    "\n",
    "        # Target trend after LMI\n",
    "        annual_mean_after = df[df['type'] == 'after'].groupby('YEAR').agg({col: ['mean', 'count']}).reset_index()\n",
    "        years = annual_mean_after['YEAR'].values.reshape(-1)\n",
    "        values = annual_mean_after[(col, 'mean')].values.reshape(-1).astype(float)\n",
    "        num_years_after = len(years)\n",
    "        \n",
    "        model_after = sm.OLS(values, sm.add_constant(years)).fit() \n",
    "        y_pred_after = model_after.predict(sm.add_constant(years))\n",
    "        slope_after = model_after.params[1]\n",
    "        intercept_after = model_after.params[0]\n",
    "        r2_after = model_after.rsquared\n",
    "        p_value_after = model_after.pvalues[1]\n",
    "        ci_after = model_after.conf_int()[1]\n",
    "        ci_width_after = (ci_after[1] - ci_after[0]) / 2\n",
    "\n",
    "        return slope, p_value, ci_width, slope_before, p_value_before, ci_width_before, slope_after, p_value_after, ci_width_after, num_years_all, num_years_before, num_years_after\n",
    "\n",
    "\n",
    "    csv_all = f'SHIPS_trends/{basin}/{basin}_all.csv'\n",
    "    csv_short = f'SHIPS_trends/{basin}/{basin}_short.csv'\n",
    "    csv_medium = f'SHIPS_trends/{basin}/{basin}_medium.csv'\n",
    "    csv_long = f'SHIPS_trends/{basin}/{basin}_long.csv'\n",
    "    \n",
    "    if os.path.exists(csv_all):\n",
    "        save_all = pd.read_csv(csv_all)\n",
    "        save_short = pd.read_csv(csv_short)\n",
    "        save_medium = pd.read_csv(csv_medium)\n",
    "        save_long = pd.read_csv(csv_long)\n",
    "    else:\n",
    "        save_all = pd.DataFrame(columns=['Target', 'slope_all', 'p_value_all', 'ci_width_all', 'slope_before', 'p_value_before', 'ci_width_before', 'slope_after', 'p_value_after', 'ci_width_after', 'num_years_all', 'num_years_before', 'num_years_after'])\n",
    "        save_short = pd.DataFrame(columns=['Target', 'slope_all', 'p_value_all', 'ci_width_all', 'slope_before', 'p_value_before', 'ci_width_before', 'slope_after', 'p_value_after', 'ci_width_after', 'num_years_all', 'num_years_before', 'num_years_after'])\n",
    "        save_medium = pd.DataFrame(columns=['Target', 'slope_all', 'p_value_all', 'ci_width_all', 'slope_before', 'p_value_before', 'ci_width_before', 'slope_after', 'p_value_after', 'ci_width_after', 'num_years_all', 'num_years_before', 'num_years_after'])\n",
    "        save_long = pd.DataFrame(columns=['Target', 'slope_all', 'p_value_all', 'ci_width_all', 'slope_before', 'p_value_before', 'ci_width_before', 'slope_after', 'p_value_after', 'ci_width_after', 'num_years_all', 'num_years_before', 'num_years_after'])\n",
    "    \n",
    "    # All TCs\n",
    "    rgr_all = predictor_trend(ships_processed, TargetName)\n",
    "    save_all.loc[len(save_all)] = [TargetName, *rgr_all]\n",
    "    save_all.to_csv(csv_all, index=False)\n",
    "\n",
    "    # Short-live TCs\n",
    "    ships_short = ships_processed[ships_processed['duration_type'] == 'short']\n",
    "    rgr_short = predictor_trend(ships_short, TargetName)\n",
    "    save_short.loc[len(save_short)] = [TargetName, *rgr_short]\n",
    "    save_short.to_csv(csv_short, index=False)\n",
    "\n",
    "    # Medium-live TCs\n",
    "    ships_medium = ships_processed[ships_processed['duration_type'] == 'medium']\n",
    "    rgr_medium = predictor_trend(ships_medium, TargetName)\n",
    "    save_medium.loc[len(save_medium)] = [TargetName, *rgr_medium]\n",
    "    save_medium.to_csv(csv_medium, index=False)\n",
    "\n",
    "    # Long-live TCs\n",
    "    ships_long = ships_processed[ships_processed['duration_type'] == 'long']\n",
    "    rgr_long = predictor_trend(ships_long, TargetName)\n",
    "    save_long.loc[len(save_long)] = [TargetName, *rgr_long]\n",
    "    save_long.to_csv(csv_long, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3fc853d",
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = ['EPAC', 'WPAC']\n",
    "variables = ['SHRD','D200','TPW','PC2','SDBT','POT','CFLX','VMPI']\n",
    "for basin in basins:\n",
    "    for variable in variables:\n",
    "        SHIPS_variable_trend(basin, variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659e0e32",
   "metadata": {},
   "source": [
    "# Table S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18fcc56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duration Group Comparison for EP Basin\n",
      "------------------------------------------------\n",
      "          Occurrence Before 2003  Occurrence of 2003 and After\n",
      "duration                                                      \n",
      "0-100h                       118                           177\n",
      "100-200h                     143                           148\n",
      ">200h                         79                            35\n",
      "\n",
      "Chi-squared test results:\n",
      "Chi-squared statistic: 28.32\n",
      "p-value: 7.08561968193573e-07\n",
      "The distributions are significantly different (p < 0.05)\n",
      "\n",
      "Duration Group Comparison for WP Basin\n",
      "------------------------------------------------\n",
      "          Occurrence Before 2003  Occurrence of 2003 and After\n",
      "duration                                                      \n",
      "0-100h                       169                           187\n",
      "100-200h                     252                           247\n",
      ">200h                        143                            90\n",
      "\n",
      "Chi-squared test results:\n",
      "Chi-squared statistic: 11.56\n",
      "p-value: 0.0030871023955596293\n",
      "The distributions are significantly different (p < 0.05)\n"
     ]
    }
   ],
   "source": [
    "def duration_group_comparison(basin, split_year):\n",
    "    IB_data = pd.read_csv('processed_data/IBTrACS_%s_processed_dt3_202510.csv'%basin)\n",
    "    IB_data['ISO_TIME'] = pd.to_datetime(IB_data['ISO_TIME'])  \n",
    "    ID_unique = sorted(IB_data['USA_ATCF_ID'].unique())        \n",
    "\n",
    "    duration_df = pd.DataFrame(columns=['year', 'ID', 'duration'])     \n",
    "    for TC_ID in ID_unique:\n",
    "        TC_data = IB_data[IB_data['USA_ATCF_ID'] == TC_ID].sort_values(by='ISO_TIME').reset_index(drop=True)\n",
    "        if len(TC_data) > 0:\n",
    "            TC_year = TC_data['YEAR'].iloc[0]\n",
    "            if len(TC_data[TC_data['COMBINE_WIND'] >= 34]) > 0:\n",
    "                start_idx = TC_data[TC_data['COMBINE_WIND'] >= 34].index[0]\n",
    "                if len(TC_data[TC_data['NATURE'] == 'TS']) > 0:    \n",
    "                    end_idx = TC_data[TC_data['NATURE'] == 'TS'].index[-1]\n",
    "                    if end_idx > start_idx:\n",
    "                        TC_duration = (TC_data['ISO_TIME'].iloc[end_idx] - TC_data['ISO_TIME'].iloc[start_idx]).total_seconds()/3600 + 3\n",
    "                        duration_df = pd.concat([duration_df, pd.DataFrame({'year':[TC_year], 'ID':[TC_ID], 'duration':[TC_duration]})], ignore_index=True)\n",
    "            \n",
    "    \n",
    "    bins = [0, 100, 200, float('inf')]\n",
    "    labels = ['0-100h', '100-200h', '>200h']\n",
    "    \n",
    "    early_period = duration_df[duration_df['year'] < split_year]\n",
    "    late_period = duration_df[duration_df['year'] >= split_year]\n",
    "\n",
    "    early_types = pd.cut(early_period['duration'], bins=bins, labels=labels)\n",
    "    late_types = pd.cut(late_period['duration'], bins=bins, labels=labels)\n",
    "\n",
    "    early_counts = early_types.value_counts().sort_index()\n",
    "    late_counts = late_types.value_counts().sort_index()\n",
    "    \n",
    "    contingency = pd.DataFrame({\n",
    "        f'Occurrence Before {split_year}': early_counts,\n",
    "        f'Occurrence of {split_year} and After': late_counts\n",
    "    })\n",
    "    \n",
    "    chi2, p_value = stats.chi2_contingency(contingency)[:2]\n",
    "    \n",
    "    \n",
    "    print(f\"\\nDuration Group Comparison for {basin} Basin\")\n",
    "    print(\"------------------------------------------------\")\n",
    "    print(contingency)\n",
    "    print(\"\\nChi-squared test results:\")\n",
    "    print(f\"Chi-squared statistic: {chi2:.2f}\")\n",
    "    print(f\"p-value: {p_value}\")\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"The distributions are significantly different (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"The distributions are not significantly different (p >= 0.05)\")\n",
    "\n",
    "duration_group_comparison('EP', 2003)\n",
    "duration_group_comparison('WP', 2003)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ca18e",
   "metadata": {},
   "source": [
    "# Table S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ac1974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP all North Occurrence slope: 0.07, p-value: 0.017, CI_width: 0.06\n",
      "EP all South Occurrence slope: -0.08, p-value: 0.072, CI_width: 0.09\n",
      "EP all North Duration slope: -1.29, p-value: 0.002, CI_width: 0.79\n",
      "EP all South Duration slope: -1.39, p-value: 0.000, CI_width: 0.69\n",
      "WP all North Occurrence slope: 0.05, p-value: 0.353, CI_width: 0.10\n",
      "WP all South Occurrence slope: -0.16, p-value: 0.002, CI_width: 0.10\n",
      "WP all North Duration slope: -0.55, p-value: 0.063, CI_width: 0.58\n",
      "WP all South Duration slope: -0.56, p-value: 0.162, CI_width: 0.80\n",
      "\n",
      "EP short North Occurrence slope: 0.09, p-value: 0.000, CI_width: 0.04\n",
      "EP short South Occurrence slope: 0.03, p-value: 0.210, CI_width: 0.05\n",
      "EP short North Duration slope: -0.55, p-value: 0.014, CI_width: 0.43\n",
      "EP short South Duration slope: -0.37, p-value: 0.075, CI_width: 0.41\n",
      "WP short North Occurrence slope: 0.06, p-value: 0.085, CI_width: 0.07\n",
      "WP short South Occurrence slope: 0.00, p-value: 0.987, CI_width: 0.04\n",
      "WP short North Duration slope: -0.18, p-value: 0.102, CI_width: 0.22\n",
      "WP short South Duration slope: -0.35, p-value: 0.191, CI_width: 0.53\n",
      "\n",
      "EP medium North Occurrence slope: -0.01, p-value: 0.413, CI_width: 0.03\n",
      "EP medium South Occurrence slope: -0.03, p-value: 0.282, CI_width: 0.06\n",
      "EP medium North Duration slope: -0.35, p-value: 0.319, CI_width: 0.71\n",
      "EP medium South Duration slope: -0.24, p-value: 0.125, CI_width: 0.31\n",
      "WP medium North Occurrence slope: 0.01, p-value: 0.700, CI_width: 0.07\n",
      "WP medium South Occurrence slope: -0.06, p-value: 0.070, CI_width: 0.07\n",
      "WP medium North Duration slope: -0.03, p-value: 0.850, CI_width: 0.27\n",
      "WP medium South Duration slope: 0.09, p-value: 0.459, CI_width: 0.23\n",
      "\n",
      "EP long North Occurrence slope: -0.00, p-value: 0.661, CI_width: 0.02\n",
      "EP long South Occurrence slope: -0.08, p-value: 0.001, CI_width: 0.04\n",
      "EP long North Duration slope: 1.91, p-value: 0.036, CI_width: 1.76\n",
      "EP long South Duration slope: -0.72, p-value: 0.206, CI_width: 1.14\n",
      "WP long North Occurrence slope: -0.02, p-value: 0.234, CI_width: 0.04\n",
      "WP long South Occurrence slope: -0.10, p-value: 0.005, CI_width: 0.07\n",
      "WP long North Duration slope: -0.04, p-value: 0.946, CI_width: 1.18\n",
      "WP long South Duration slope: 0.18, p-value: 0.650, CI_width: 0.78\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def north_south_trends(basin, duration_type):\n",
    "    IB_data = pd.read_csv('processed_data/IBTrACS_%s_processed_dt3_202510.csv'%basin)\n",
    "    IB_data['ISO_TIME'] = pd.to_datetime(IB_data['ISO_TIME'])  \n",
    "    ID_unique = sorted(IB_data['USA_ATCF_ID'].unique())        \n",
    "\n",
    "    TC_info_df = pd.DataFrame(columns=['year', 'ID', 'duration', 'LMI', 'start_lat', 'start_lon'])     \n",
    "    for TC_ID in ID_unique:\n",
    "        TC_data = IB_data[IB_data['USA_ATCF_ID'] == TC_ID].sort_values(by='ISO_TIME').reset_index(drop=True)\n",
    "        if len(TC_data) > 0:\n",
    "            TC_year = TC_data['YEAR'].iloc[0]\n",
    "            if len(TC_data[TC_data['COMBINE_WIND'] >= 34]) > 0:\n",
    "                start_idx = TC_data[TC_data['COMBINE_WIND'] >= 34].index[0]\n",
    "                start_lat = TC_data['LAT'].iloc[start_idx]\n",
    "                start_lon = TC_data['LON'].iloc[start_idx]\n",
    "                if len(TC_data[TC_data['NATURE'] == 'TS']) > 0:    \n",
    "                    end_idx = TC_data[TC_data['NATURE'] == 'TS'].index[-1]\n",
    "                    if end_idx > start_idx:\n",
    "                        TC_duration = (TC_data['ISO_TIME'].iloc[end_idx] - TC_data['ISO_TIME'].iloc[start_idx]).total_seconds()/3600 + 3\n",
    "                        TC_valid = TC_data.iloc[start_idx:end_idx+1].reset_index(drop=True)\n",
    "                        if TC_duration < 100:\n",
    "                            TC_type = 'short'\n",
    "                        elif TC_duration < 200:\n",
    "                            TC_type = 'medium'\n",
    "                        else:\n",
    "                            TC_type = 'long'\n",
    "                        TC_info_df = pd.concat([TC_info_df, pd.DataFrame({'year':[TC_year], 'ID':[TC_ID], 'duration':[TC_duration], 'start_lat':[start_lat], 'type':[TC_type]})], ignore_index=True)\n",
    "\n",
    "    if duration_type == 'all':\n",
    "        pass\n",
    "    else:\n",
    "        TC_info_df = TC_info_df[TC_info_df['type'] == duration_type]\n",
    "\n",
    "    years = sorted(TC_info_df['year'].unique())\n",
    "    divide_lat = {'EP':15, 'WP':15}\n",
    "    basin_lat = divide_lat[basin]\n",
    "    count_north = []\n",
    "    count_south = []\n",
    "    duration_north = []\n",
    "    duration_south = []\n",
    "    for year in years:\n",
    "        TC_info_df_year = TC_info_df[TC_info_df['year'] == year]\n",
    "        north_year = TC_info_df_year[TC_info_df_year['start_lat'] > basin_lat]\n",
    "        south_year = TC_info_df_year[TC_info_df_year['start_lat'] <= basin_lat]\n",
    "        count_north.append((year, len(north_year)))\n",
    "        count_south.append((year, len(south_year)))\n",
    "        duration_north.append((year, np.mean(north_year['duration'])))\n",
    "        duration_south.append((year, np.mean(south_year['duration'])))\n",
    "    count_north = sorted(count_north, key=lambda x: x[0])\n",
    "    count_south = sorted(count_south, key=lambda x: x[0])\n",
    "    years_count_north, counts_north = zip(*count_north)\n",
    "    years_count_south, counts_south = zip(*count_south)\n",
    "\n",
    "    duration_north = sorted(duration_north, key=lambda x: x[0])\n",
    "    duration_south = sorted(duration_south, key=lambda x: x[0])\n",
    "    # Remove NaN values from duration lists\n",
    "    duration_north = [(year, dur) for year, dur in duration_north if not np.isnan(dur)]\n",
    "    duration_south = [(year, dur) for year, dur in duration_south if not np.isnan(dur)]\n",
    "    years_duration_north, durations_north = zip(*duration_north)     \n",
    "    years_duration_south, durations_south = zip(*duration_south)\n",
    "    \n",
    "    def trend_by_region(year_list, value_list, region, variable):\n",
    "        X = sm.add_constant(year_list)\n",
    "        model = sm.OLS(value_list, X).fit()\n",
    "        y_pred = model.predict(X)\n",
    "        slope = model.params[1]\n",
    "        intercept = model.params[0]\n",
    "        r2 = model.rsquared \n",
    "        p_value = model.pvalues[1]\n",
    "        ci = model.conf_int()[1]\n",
    "        print(f'{basin} {duration_type} {region} {variable} slope: {slope:.2f}, p-value: {p_value:.3f}, CI_width: {(ci[1]-ci[0])/2:.2f}')\n",
    "        \n",
    "\n",
    "    trend_by_region(years_count_north, counts_north, 'North', 'Occurrence')\n",
    "    trend_by_region(years_count_south, counts_south, 'South', 'Occurrence')\n",
    "    trend_by_region(years_duration_north, durations_north, 'North', 'Duration')\n",
    "    trend_by_region(years_duration_south, durations_south, 'South', 'Duration')\n",
    "\n",
    "types = ['all', 'short', 'medium', 'long']\n",
    "for duration_type in types:\n",
    "    north_south_trends('EP', duration_type)\n",
    "    north_south_trends('WP', duration_type)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_s",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
